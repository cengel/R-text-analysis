<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Analyzing Texts | Text Analysis with R</title>
  <meta name="description" content="Workshop materials for Text Analysis in R" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Analyzing Texts | Text Analysis with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Workshop materials for Text Analysis in R" />
  <meta name="github-repo" content="cengel/R-text-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Analyzing Texts | Text Analysis with R" />
  
  <meta name="twitter:description" content="Workshop materials for Text Analysis in R" />
  

<meta name="author" content="Claudia Engel, Scott Bailey" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="textprep.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Text Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="textprep.html"><a href="textprep.html"><i class="fa fa-check"></i><b>1</b> Preparing Textual Data</a><ul>
<li class="chapter" data-level="1.1" data-path="textprep.html"><a href="textprep.html#reading-text-into-r"><i class="fa fa-check"></i><b>1.1</b> Reading text into R</a></li>
<li class="chapter" data-level="1.2" data-path="textprep.html"><a href="textprep.html#string-operations"><i class="fa fa-check"></i><b>1.2</b> String operations</a></li>
<li class="chapter" data-level="1.3" data-path="textprep.html"><a href="textprep.html#tokenize-lowercase"><i class="fa fa-check"></i><b>1.3</b> Tokenize, lowercase</a></li>
<li class="chapter" data-level="1.4" data-path="textprep.html"><a href="textprep.html#stopwords"><i class="fa fa-check"></i><b>1.4</b> Stopwords</a></li>
<li class="chapter" data-level="1.5" data-path="textprep.html"><a href="textprep.html#word-stemming"><i class="fa fa-check"></i><b>1.5</b> Word Stemming</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="textanalysis.html"><a href="textanalysis.html"><i class="fa fa-check"></i><b>2</b> Analyzing Texts</a><ul>
<li class="chapter" data-level="2.1" data-path="textanalysis.html"><a href="textanalysis.html#frequencies"><i class="fa fa-check"></i><b>2.1</b> Frequencies</a></li>
<li class="chapter" data-level="2.2" data-path="textanalysis.html"><a href="textanalysis.html#term-frequency"><i class="fa fa-check"></i><b>2.2</b> Term frequency</a></li>
<li class="chapter" data-level="2.3" data-path="textanalysis.html"><a href="textanalysis.html#tf-idf"><i class="fa fa-check"></i><b>2.3</b> Tf-idf</a></li>
<li class="chapter" data-level="2.4" data-path="textanalysis.html"><a href="textanalysis.html#n-grams"><i class="fa fa-check"></i><b>2.4</b> N-Grams</a></li>
<li class="chapter" data-level="2.5" data-path="textanalysis.html"><a href="textanalysis.html#co-occurrence"><i class="fa fa-check"></i><b>2.5</b> Co-occurrence</a></li>
<li class="chapter" data-level="2.6" data-path="textanalysis.html"><a href="textanalysis.html#document-term-matrix"><i class="fa fa-check"></i><b>2.6</b> Document-Term Matrix</a></li>
<li class="chapter" data-level="2.7" data-path="textanalysis.html"><a href="textanalysis.html#sentiment-analysis"><i class="fa fa-check"></i><b>2.7</b> Sentiment analysis</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="textanalysis" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Analyzing Texts</h1>
<blockquote>
<p>Learning Objectives</p>
<ul>
<li>perform basic text analysis operations in R</li>
<li>determine differnd kinds of frequency counts</li>
<li>use the <code>widyr</code> package to calculate co-ocurrance</li>
<li>use <code>igraph</code> and <code>ggraph</code> to plot a co-ocurrance graph</li>
<li>import and export a Document-Term Matrix into <code>tidytext</code></li>
<li>use the <code>sentiments</code> dataset from <code>tidytext</code> to perform a sentiment analysis</li>
</ul>
</blockquote>
<hr />
<p>Now that we’ve read in our text and metadata, tokenized and cleaned it a little, let’s move on to some analysis.</p>
<p>First, we’ll make sure we have loaded the libraries we’ll need.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidytext)</code></pre>
<p>Let’s remind ourselves of what our data looks like.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words</code></pre>
<pre><code>#&gt; # A tibble: 778,161 x 7
#&gt;    president     year years_active party   sotu_type doc_id        word    
#&gt;    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;   
#&gt;  1 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… fellow  
#&gt;  2 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… citizens
#&gt;  3 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… senate  
#&gt;  4 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… house   
#&gt;  5 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… represe…
#&gt;  6 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… midst   
#&gt;  7 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… unprece…
#&gt;  8 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… politic…
#&gt;  9 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… troubles
#&gt; 10 Abraham Lin…  1861 1861-1865    Republ… written   abraham-linc… gratitu…
#&gt; # … with 778,151 more rows</code></pre>
<div id="frequencies" class="section level2">
<h2><span class="header-section-number">2.1</span> Frequencies</h2>
<p>Since our unit of analysis at this point is a word, let’s do some straightforward counting to figure out which words occur most frequently in the corpus as a whole.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code>#&gt; # A tibble: 29,558 x 2
#&gt;    word           n
#&gt;    &lt;chr&gt;      &lt;int&gt;
#&gt;  1 government  7573
#&gt;  2 congress    5759
#&gt;  3 united      5102
#&gt;  4 people      4219
#&gt;  5 country     3564
#&gt;  6 public      3413
#&gt;  7 time        3138
#&gt;  8 war         2961
#&gt;  9 american    2853
#&gt; 10 world       2581
#&gt; # … with 29,548 more rows</code></pre>
<p>We could start adding in a bit of visualization here. Let’s show the most frequent words that occur more than 2000 times.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">2000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, n)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>What if we’re interested in most used words per speech?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Count words by book</span>
doc_words &lt;-<span class="st"> </span>tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)

<span class="co"># Calculate the total number of words by book and save them to a tibble</span>
total_words &lt;-<span class="st"> </span>doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">total =</span> <span class="kw">sum</span>(n))

<span class="co"># Join the total column with the rest of the data so we can calculate frequency</span>
doc_words &lt;-<span class="st"> </span><span class="kw">left_join</span>(doc_words, total_words)

doc_words </code></pre>
<pre><code>#&gt; # A tibble: 352,846 x 4
#&gt;    doc_id                       word               n total
#&gt;    &lt;chr&gt;                        &lt;chr&gt;          &lt;int&gt; &lt;int&gt;
#&gt;  1 harry-s-truman-1946.txt      dollars          207 12614
#&gt;  2 jimmy-carter-1980b.txt       congress         204 16128
#&gt;  3 harry-s-truman-1946.txt      war              201 12614
#&gt;  4 william-howard-taft-1910.txt government       164 11178
#&gt;  5 james-k-polk-1846.txt        mexico           158  7023
#&gt;  6 richard-m-nixon-1974b.txt    federal          141  9996
#&gt;  7 harry-s-truman-1946.txt      million          138 12614
#&gt;  8 harry-s-truman-1946.txt      fiscal           129 12614
#&gt;  9 jimmy-carter-1981.txt        administration   129 16595
#&gt; 10 william-howard-taft-1912.txt government       129 10215
#&gt; # … with 352,836 more rows</code></pre>
<p>Let’s graph the top words per book.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n, <span class="dt">fill =</span> doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>That’s cool looking, but let’s split it into facets so we can see by speech.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n, <span class="dt">fill =</span> doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>doc_id, <span class="dt">ncol =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We could keep cleaning this figure up by setting some minimum sizing, determining the spacing between y-axis labels better, and so forth, but for now we’ll accept it as showing some sense of variation across speeches where certain words are used most.</p>
<p>What if we want to check the most common words per speech for a single president? We could filter this <code>doc_words</code> dataset based on the president’s name being in the doc_id, but I think it’s easier to filter from the initial tidy data and recount.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(president <span class="op">==</span><span class="st"> &quot;Barack Obama&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">20</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n, <span class="dt">fill=</span>doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>doc_id, <span class="dt">ncol =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="term-frequency" class="section level2">
<h2><span class="header-section-number">2.2</span> Term frequency</h2>
<p>Sometimes, a raw count of a word is less important than understanding how often that word appears in respect to the total number of words in a text. This ratio would be the <strong>term frequency</strong>.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words &lt;-<span class="st"> </span>doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">term_freq =</span> n <span class="op">/</span><span class="st"> </span>total)

doc_words </code></pre>
<pre><code>#&gt; # A tibble: 352,846 x 5
#&gt;    doc_id                       word               n total term_freq
#&gt;    &lt;chr&gt;                        &lt;chr&gt;          &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;
#&gt;  1 harry-s-truman-1946.txt      dollars          207 12614   0.0164 
#&gt;  2 jimmy-carter-1980b.txt       congress         204 16128   0.0126 
#&gt;  3 harry-s-truman-1946.txt      war              201 12614   0.0159 
#&gt;  4 william-howard-taft-1910.txt government       164 11178   0.0147 
#&gt;  5 james-k-polk-1846.txt        mexico           158  7023   0.0225 
#&gt;  6 richard-m-nixon-1974b.txt    federal          141  9996   0.0141 
#&gt;  7 harry-s-truman-1946.txt      million          138 12614   0.0109 
#&gt;  8 harry-s-truman-1946.txt      fiscal           129 12614   0.0102 
#&gt;  9 jimmy-carter-1981.txt        administration   129 16595   0.00777
#&gt; 10 william-howard-taft-1912.txt government       129 10215   0.0126 
#&gt; # … with 352,836 more rows</code></pre>
<p>Let’s graph the term frequency for one of these speeches so we can understand the frequency distribution of words over a text.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(doc_id <span class="op">==</span><span class="st"> &quot;harry-s-truman-1946.txt&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(term_freq)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlim</span>(<span class="ot">NA</span>, <span class="fl">.012</span>)</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>This distribution makes sense. Most words are used relatively rarely in a text. Only a few have a high term frequency.</p>
<p>We could keep filtering this data to see which terms have high frequency, thus maybe increased significance, for different presidents and different particular speeches. We could also subset based on decade, and get a sense of what was important in each decade. We’re going to take a slightly different approach though. We’ve been looking at term frequency per document. What if we want to know about words that seem more important based on the contents of the entire corpus?</p>
</div>
<div id="tf-idf" class="section level2">
<h2><span class="header-section-number">2.3</span> Tf-idf</h2>
<p>For this, we can use term-frequency according to inverse document frequency (tf-idf). Tf-idf measures how important a word is within a corpus by scaling term frequency per document according to the inverse of the term’s document frequency (number of documents within the corpus in which the term appears divided by the number of documents).</p>
<p>We could write our own function for tf-idf, but in this case we’ll take advantage of tidytext’s implementation.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words &lt;-<span class="st"> </span>doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, doc_id, n)

doc_words</code></pre>
<pre><code>#&gt; # A tibble: 352,846 x 8
#&gt;    doc_id           word          n total term_freq      tf     idf  tf_idf
#&gt;    &lt;chr&gt;            &lt;chr&gt;     &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 harry-s-truman-… dollars     207 12614   0.0164  0.0164  0.612   1.00e-2
#&gt;  2 jimmy-carter-19… congress    204 16128   0.0126  0.0126  0.00425 5.37e-5
#&gt;  3 harry-s-truman-… war         201 12614   0.0159  0.0159  0.0345  5.50e-4
#&gt;  4 william-howard-… governme…   164 11178   0.0147  0.0147  0.00425 6.23e-5
#&gt;  5 james-k-polk-18… mexico      158  7023   0.0225  0.0225  0.810   1.82e-2
#&gt;  6 richard-m-nixon… federal     141  9996   0.0141  0.0141  0.293   4.14e-3
#&gt;  7 harry-s-truman-… million     138 12614   0.0109  0.0109  0.728   7.96e-3
#&gt;  8 harry-s-truman-… fiscal      129 12614   0.0102  0.0102  0.494   5.05e-3
#&gt;  9 jimmy-carter-19… administ…   129 16595   0.00777 0.00777 0.282   2.19e-3
#&gt; 10 william-howard-… governme…   129 10215   0.0126  0.0126  0.00425 5.36e-5
#&gt; # … with 352,836 more rows</code></pre>
<p>The tf-idf value will be:</p>
<ul>
<li>lower for words that appear in many documents in the corpus, and lowest when the word occurs in virtually all documents.</li>
<li>high for words that appear many times in few documents in the corpus, this lending high discriminatory power to those documents.</li>
</ul>
<p>Let’s look at some of the words in the corpus that have the highest tf-idf scores, which means words that are particularly distinctive for their documents.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>total) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))</code></pre>
<pre><code>#&gt; # A tibble: 352,846 x 7
#&gt;    doc_id                     word         n term_freq      tf   idf tf_idf
#&gt;    &lt;chr&gt;                      &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
#&gt;  1 lyndon-b-johnson-1966.txt  vietnam     32   0.0152  0.0152   2.42 0.0367
#&gt;  2 jimmy-carter-1980a.txt     soviet      31   0.0218  0.0218   1.47 0.0321
#&gt;  3 george-w-bush-2003.txt     hussein     19   0.00811 0.00811  3.85 0.0313
#&gt;  4 george-w-bush-2003.txt     saddam      19   0.00811 0.00811  3.67 0.0298
#&gt;  5 franklin-d-roosevelt-1943… 1942        13   0.00758 0.00758  3.85 0.0292
#&gt;  6 dwight-d-eisenhower-1961.… 1953        23   0.00747 0.00747  3.85 0.0288
#&gt;  7 john-adams-1800.txt        gentlem…     8   0.0153  0.0153   1.80 0.0275
#&gt;  8 benjamin-harrison-1892.txt 1892        40   0.00741 0.00741  3.52 0.0261
#&gt;  9 franklin-d-roosevelt-1942… hitler       7   0.00527 0.00527  4.77 0.0251
#&gt; 10 herbert-hoover-1930.txt    1928        14   0.00711 0.00711  3.52 0.0250
#&gt; # … with 352,836 more rows</code></pre>
<p>These results seem appropriate given our history. To understand the occurrence of the years we might need to look more closely at the speeches themselves, and determine whether the years are significant or whether they need to be removed from the text. It might be that even if they don’t need to be removed from the text overall, they still need to be filtered out within the context of this analysis.</p>
<p>In the same way that we narrowed our analysis to Obama speeches earlier, we could subset the corpus before we calculate the tf-idf score to understand which words are most important for a single president within their sotu speeches. Let’s do that for Obama.</p>
<pre class="sourceCode r"><code class="sourceCode r">obama_tf_idf &lt;-<span class="st"> </span>tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(president <span class="op">==</span><span class="st"> &quot;Barack Obama&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, doc_id, n) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))

obama_tf_idf</code></pre>
<pre><code>#&gt; # A tibble: 10,656 x 6
#&gt;    doc_id                word          n      tf   idf  tf_idf
#&gt;    &lt;chr&gt;                 &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 barack-obama-2016.txt voices        8 0.00372  2.08 0.00773
#&gt;  2 barack-obama-2014.txt cory          9 0.00322  2.08 0.00671
#&gt;  3 barack-obama-2015.txt rebekah       7 0.00273  2.08 0.00567
#&gt;  4 barack-obama-2012.txt unit          7 0.00255  2.08 0.00531
#&gt;  5 barack-obama-2016.txt isil          8 0.00372  1.39 0.00515
#&gt;  6 barack-obama-2009.txt restart       5 0.00221  2.08 0.00460
#&gt;  7 barack-obama-2013.txt reduction     6 0.00220  2.08 0.00458
#&gt;  8 barack-obama-2015.txt childcare     8 0.00312  1.39 0.00432
#&gt;  9 barack-obama-2011.txt brandon       5 0.00197  2.08 0.00409
#&gt; 10 barack-obama-2015.txt economics     5 0.00195  2.08 0.00405
#&gt; # … with 10,646 more rows</code></pre>
<p>Based on what you know of the Obama years and sotu speeches generally, how would you interpret these results?</p>
<p>Let’s try graphing these results, showing the top tf-idf terms per speech for Obama’s speeches.</p>
<pre class="sourceCode r"><code class="sourceCode r">obama_tf_idf <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">factor</span>(word, <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">unique</span>(word)))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">5</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, tf_idf, <span class="dt">fill =</span> doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;tf-idf&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>doc_id, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.y =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">45</span>)) </code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="n-grams" class="section level2">
<h2><span class="header-section-number">2.4</span> N-Grams</h2>
<p>We mentioned n-grams in the intro, but let’s revisit them here and take a look at the most common bigrams in the speeches. Remember this is what we get back:</p>
<pre class="sourceCode r"><code class="sourceCode r">sotu_whole <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest_tokens</span>(bigram, text, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>) <span class="co"># create bigram</span></code></pre>
<pre><code>#&gt; # A tibble: 1,964,976 x 7
#&gt;    president     year years_active party  sotu_type doc_id       bigram    
#&gt;    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;     
#&gt;  1 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… fellow ci…
#&gt;  2 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… citizens …
#&gt;  3 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… of the    
#&gt;  4 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… the senate
#&gt;  5 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… senate and
#&gt;  6 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… and house 
#&gt;  7 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… house of  
#&gt;  8 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… of repres…
#&gt;  9 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… represent…
#&gt; 10 Abraham Lin…  1861 1861-1865    Repub… written   abraham-lin… in the    
#&gt; # … with 1,964,966 more rows</code></pre>
<p>Let’s see the most common bigrams:</p>
<pre class="sourceCode r"><code class="sourceCode r">sotu_whole <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest_tokens</span>(bigram, text, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(bigram, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="co"># count ocurrences and sord descending</span></code></pre>
<pre><code>#&gt; # A tibble: 469,092 x 2
#&gt;    bigram            n
#&gt;    &lt;chr&gt;         &lt;int&gt;
#&gt;  1 of the        33610
#&gt;  2 in the        12499
#&gt;  3 to the        11643
#&gt;  4 for the        6892
#&gt;  5 and the        6224
#&gt;  6 by the         5606
#&gt;  7 of our         5172
#&gt;  8 the united     4767
#&gt;  9 united states  4760
#&gt; 10 it is          4756
#&gt; # … with 469,082 more rows</code></pre>
<p>Ok, so we again need to remove the stopwords. This time let’s use dplyr’s <code>filter</code> function for this. And before that we will <code>separate</code> the two words into two columns.</p>
<pre class="sourceCode r"><code class="sourceCode r">sotu_bigrams &lt;-<span class="st"> </span>sotu_whole <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest_tokens</span>(bigram, text, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">separate</span>(bigram, <span class="kw">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word2&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># separate into cols</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word1 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># remove stopwords</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word2 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word)

sotu_bigrams <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(word1, word2, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code>#&gt; # A tibble: 129,622 x 3
#&gt;    word1    word2          n
#&gt;    &lt;chr&gt;    &lt;chr&gt;      &lt;int&gt;
#&gt;  1 federal  government   479
#&gt;  2 american people       428
#&gt;  3 june     30           325
#&gt;  4 fellow   citizens     296
#&gt;  5 public   debt         283
#&gt;  6 public   lands        256
#&gt;  7 health   care         240
#&gt;  8 social   security     232
#&gt;  9 post     office       202
#&gt; 10 annual   message      200
#&gt; # … with 129,612 more rows</code></pre>
<p>(Bonus question: What happened on that June 30th?)</p>
<p>A bigram can also be treated as a term in a document in the same way that we treated individual words. That means we can look at tf-idf values in the same way.</p>
<p>First we will re-unite the two word columns again, and then generate the tf-idf count as above.</p>
<pre class="sourceCode r"><code class="sourceCode r">bigram_tf_idf &lt;-<span class="st"> </span>sotu_bigrams <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unite</span>(bigram, word1, word2, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># combine columns</span>
<span class="st">  </span><span class="kw">count</span>(president, bigram) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_tf_idf</span>(bigram, president, n) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))</code></pre>
<p>What makes the speeches of different presidents unique?</p>
<p>Let’s pick a few presidents and plot their highest scoring tf-idf values here.</p>
<pre class="sourceCode r"><code class="sourceCode r">potus &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;John F. Kennedy&quot;</span>, <span class="st">&quot;Richard M. Nixon&quot;</span>, <span class="st">&quot;George Bush&quot;</span>, <span class="st">&quot;George W. Bush&quot;</span>)

bigram_tf_idf <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(president <span class="op">%in%</span><span class="st"> </span>potus) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(president) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">20</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">reorder</span>(bigram, tf_idf), tf_idf, <span class="dt">fill =</span> president)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;tf-idf&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>president, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">nrow =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="R-text-analysis_files/figure-html/bigram-tf-idf-plot-1.png" width="672" /></p>
</div>
<div id="co-occurrence" class="section level2">
<h2><span class="header-section-number">2.5</span> Co-occurrence</h2>
<p>Co-occurrences give us a sense of words that appear in the same text, but not necessarily next to each other.</p>
<p>For this section we will make use of the <code>widyr</code> package. It allows us to turn our table into a wide matrix. In our case that matrix will be made up of the individual words and the cell values will be the counts of how many times they co-occur. Then we will turn the matrix back into a tidy form, where each row contains the word pairs and the count of their co-occurrence. This lets us count common pairs of words co-appearing within the same speech.</p>
<p>The function which helps us do this is the <code>pairwise_count()</code> function.</p>
<p>Since processing the entire corpus would take too long here, we will only look at the last 20 words of each speech.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(widyr)

<span class="co"># extract last 100 words from text</span>
sotu_whole<span class="op">$</span>speech_end &lt;-<span class="st"> </span><span class="kw">word</span>(sotu_whole<span class="op">$</span>text, <span class="dv">-100</span>, <span class="dt">end =</span> <span class="dv">-1</span>)

sotu_word_pairs &lt;-<span class="st"> </span>sotu_whole <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unnest_tokens</span>(word, speech_end) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># remove stopwords</span>
<span class="st">  </span><span class="kw">pairwise_count</span>(word, doc_id, <span class="dt">sort =</span> <span class="ot">TRUE</span>, <span class="dt">upper =</span> <span class="ot">FALSE</span>) <span class="co"># don&#39;t include upper triangle of matrix</span>

sotu_word_pairs</code></pre>
<pre><code>#&gt; # A tibble: 125,576 x 3
#&gt;    item1      item2       n
#&gt;    &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt;
#&gt;  1 god        bless      37
#&gt;  2 god        america    35
#&gt;  3 bless      america    30
#&gt;  4 people     country    26
#&gt;  5 world      god        22
#&gt;  6 god        people     22
#&gt;  7 government people     21
#&gt;  8 congress   people     21
#&gt;  9 public     country    21
#&gt; 10 god        nation     21
#&gt; # … with 125,566 more rows</code></pre>
<p>To plot the co-occurrence network, we use the <code>igraph</code> library to convert our table into a network graph and <code>ggraph</code> which adds functionality to ggplot and makes it easier to create a network plot.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(igraph)
<span class="kw">library</span>(ggraph)

sotu_word_pairs <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># only word pairs that occur 10 or more times</span>
<span class="st">  </span><span class="kw">graph_from_data_frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#convert to graph</span>
<span class="st">  </span><span class="kw">ggraph</span>(<span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="co"># place nodes according to the force-directed algorithm of Fruchterman and Reingold</span>
<span class="st">  </span><span class="kw">geom_edge_link</span>(<span class="kw">aes</span>(<span class="dt">edge_alpha =</span> n, <span class="dt">edge_width =</span> n), <span class="dt">edge_colour =</span> <span class="st">&quot;tomato&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_node_point</span>(<span class="dt">size =</span> <span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">repel =</span> <span class="ot">TRUE</span>, 
                 <span class="dt">point.padding =</span> <span class="kw">unit</span>(<span class="fl">0.2</span>, <span class="st">&quot;lines&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_void</span>()</code></pre>
<p><img src="R-text-analysis_files/figure-html/plot-network-1.png" width="672" /></p>
<p>There are alternative approaches for this as well. See for example the <code>findAssocs</code> function in the <code>tm</code> package.</p>
</div>
<div id="document-term-matrix" class="section level2">
<h2><span class="header-section-number">2.6</span> Document-Term Matrix</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Document-term_matrix">document-term matrix (DTM)</a> is a format which is frequently used in text analysis. It is a matrix where we can see the counts of each term per document. In a DTM each row represents a document, each column represents a term, and the cell values are the counts of the occurrences of the term for the particular document.</p>
<p><code>tidytext</code> provides functionality to convert to and from DTMs, if for example, your analyis requires specific functions that require you to use a different R package which only works with DTM objects.</p>
<p>The <code>cast_dtm</code> function can be used to create a DTM object from a tidy table.</p>
<p>Let’s assume that for some reason we want to use the <code>findAssoc</code> function from the <code>tm</code> package.</p>
<p>First we use dplyr to create a table with the document name, the term, and the count.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># make a table with document, term, count</span>
tidy_sotu_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word) </code></pre>
<pre><code>#&gt; # A tibble: 352,846 x 3
#&gt;    doc_id                   word               n
#&gt;    &lt;chr&gt;                    &lt;chr&gt;          &lt;int&gt;
#&gt;  1 abraham-lincoln-1861.txt 1,470,018          1
#&gt;  2 abraham-lincoln-1861.txt 1,500              1
#&gt;  3 abraham-lincoln-1861.txt 100,000            1
#&gt;  4 abraham-lincoln-1861.txt 102,532,509.27     1
#&gt;  5 abraham-lincoln-1861.txt 12,528,000         1
#&gt;  6 abraham-lincoln-1861.txt 13,606,759.11      1
#&gt;  7 abraham-lincoln-1861.txt 1830               1
#&gt;  8 abraham-lincoln-1861.txt 1859               1
#&gt;  9 abraham-lincoln-1861.txt 1860               2
#&gt; 10 abraham-lincoln-1861.txt 1861               6
#&gt; # … with 352,836 more rows</code></pre>
<p>Now we cast it as a DTM.</p>
<pre class="sourceCode r"><code class="sourceCode r">sotu_dtm &lt;-<span class="st"> </span>tidy_sotu_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">cast_dtm</span>(doc_id, word, n) 

<span class="kw">class</span>(sotu_dtm)</code></pre>
<pre><code>#&gt; [1] &quot;DocumentTermMatrix&quot;    &quot;simple_triplet_matrix&quot;</code></pre>
<p>Finally, let’s use it in the <code>tm</code> package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tm)

<span class="co"># look at the terms with tm function</span>
<span class="kw">Terms</span>(sotu_dtm) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tail</span>()</code></pre>
<pre><code>#&gt; [1] &quot;queretaro&quot;    &quot;refreshments&quot; &quot;schleswig&quot;    &quot;sedulous&quot;    
#&gt; [5] &quot;subagents&quot;    &quot;transcript&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># most frequent terms</span>
<span class="kw">findFreqTerms</span>(sotu_dtm, <span class="dt">lowfreq =</span> <span class="dv">5000</span>)</code></pre>
<pre><code>#&gt; [1] &quot;congress&quot;   &quot;government&quot; &quot;united&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># find terms associated with ...</span>
<span class="kw">findAssocs</span>(sotu_dtm, <span class="st">&quot;citizen&quot;</span>, <span class="dt">corlimit =</span> <span class="fl">0.5</span>)</code></pre>
<pre><code>#&gt; $citizen
#&gt;        laws citizenship  protection   contained    entitled  government 
#&gt;        0.62        0.59        0.56        0.55        0.53        0.53 
#&gt;    citizens  postmaster     careful    question      report       suits 
#&gt;        0.52        0.52        0.51        0.51        0.51        0.51</code></pre>
<p>Conversely, <code>tidytext</code> implements the <code>tidy</code> function (originally from the <code>broom</code> package) to import DocumentTermMatrix objects. Note that it only takes the cells from the DTM that are not 0, so there will be no rows with 0 counts.</p>
</div>
<div id="sentiment-analysis" class="section level2">
<h2><span class="header-section-number">2.7</span> Sentiment analysis</h2>
<p><code>tidytext</code> comes with a dataset <code>sentiments</code> which contains several sentiment lexicons, where each word is attributed a certain sentiment, like this:</p>
<pre class="sourceCode r"><code class="sourceCode r">sentiments</code></pre>
<pre><code>#&gt; # A tibble: 6,786 x 2
#&gt;    word        sentiment
#&gt;    &lt;chr&gt;       &lt;chr&gt;    
#&gt;  1 2-faces     negative 
#&gt;  2 abnormal    negative 
#&gt;  3 abolish     negative 
#&gt;  4 abominable  negative 
#&gt;  5 abominably  negative 
#&gt;  6 abominate   negative 
#&gt;  7 abomination negative 
#&gt;  8 abort       negative 
#&gt;  9 aborted     negative 
#&gt; 10 aborts      negative 
#&gt; # … with 6,776 more rows</code></pre>
<p>Here we will take a look at how the sentiment of the speeches change over time. We will use the lexicon from <a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">Bing Liu and collaborators</a>, which assigns positive/negative labels for each word:</p>
<pre class="sourceCode r"><code class="sourceCode r">bing_lex &lt;-<span class="st"> </span><span class="kw">get_sentiments</span>(<span class="st">&quot;bing&quot;</span>)
bing_lex</code></pre>
<pre><code>#&gt; # A tibble: 6,786 x 2
#&gt;    word        sentiment
#&gt;    &lt;chr&gt;       &lt;chr&gt;    
#&gt;  1 2-faces     negative 
#&gt;  2 abnormal    negative 
#&gt;  3 abolish     negative 
#&gt;  4 abominable  negative 
#&gt;  5 abominably  negative 
#&gt;  6 abominate   negative 
#&gt;  7 abomination negative 
#&gt;  8 abort       negative 
#&gt;  9 aborted     negative 
#&gt; 10 aborts      negative 
#&gt; # … with 6,776 more rows</code></pre>
<p>Since this is a regular tibble, we can use these sentiments and join them to the words of our speeches. We will use <code>inner_join</code> from <code>dplyr</code>. Since our columns to join on have the same name (<code>word</code>) we don’t need to explicitly name it.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">inner_join</span>(bing_lex) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># join</span>
<span class="st">  </span><span class="kw">count</span>(year, sentiment) <span class="co"># group by year and sentiment</span></code></pre>
<pre><code>#&gt; # A tibble: 450 x 3
#&gt;     year sentiment     n
#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;int&gt;
#&gt;  1  1790 negative     39
#&gt;  2  1790 positive    125
#&gt;  3  1791 negative     52
#&gt;  4  1791 positive    103
#&gt;  5  1792 negative     57
#&gt;  6  1792 positive     78
#&gt;  7  1793 negative     58
#&gt;  8  1793 positive     72
#&gt;  9  1794 negative    110
#&gt; 10  1794 positive    106
#&gt; # … with 440 more rows</code></pre>
<p>Finally we can visualize it like this:</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">inner_join</span>(bing_lex) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># join</span>
<span class="st">  </span><span class="kw">count</span>(year, sentiment) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># group by year and sentiment</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(year, n, <span class="dt">color =</span> sentiment)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">1790</span>, <span class="dv">2016</span>, <span class="dt">by =</span> <span class="dv">10</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">45</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))</code></pre>
<p><img src="R-text-analysis_files/figure-html/sentiment-plot-1.png" width="576" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="textprep.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cengel/R-text-analysis/edit/master/02-data-analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["R-text-analysis.pdf", "R-text-analysis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
