<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Preparing Textual Data | Text Analysis with R</title>
  <meta name="description" content="Workshop materials for Text Analysis in R">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Preparing Textual Data | Text Analysis with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Workshop materials for Text Analysis in R" />
  <meta name="github-repo" content="cengel/R-text-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Preparing Textual Data | Text Analysis with R" />
  
  <meta name="twitter:description" content="Workshop materials for Text Analysis in R" />
  

<meta name="author" content="Claudia Engel, Scott Bailey">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="textanalysis.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Text Analysis with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="textanalysis.html"><a href="textanalysis.html"><i class="fa fa-check"></i><b>1</b> Analysing Texts</a><ul>
<li class="chapter" data-level="1.1" data-path="textanalysis.html"><a href="textanalysis.html#reading-text-into-r"><i class="fa fa-check"></i><b>1.1</b> Reading text into R</a></li>
<li class="chapter" data-level="1.2" data-path="textanalysis.html"><a href="textanalysis.html#string-operations"><i class="fa fa-check"></i><b>1.2</b> String operations</a></li>
<li class="chapter" data-level="1.3" data-path="textanalysis.html"><a href="textanalysis.html#tokenize-lowercase"><i class="fa fa-check"></i><b>1.3</b> Tokenize, lowercase</a></li>
<li class="chapter" data-level="1.4" data-path="textanalysis.html"><a href="textanalysis.html#stopwords"><i class="fa fa-check"></i><b>1.4</b> Stopwords</a></li>
<li class="chapter" data-level="1.5" data-path="textanalysis.html"><a href="textanalysis.html#stemming-and-lemmatization"><i class="fa fa-check"></i><b>1.5</b> Stemming and Lemmatization?</a></li>
<li class="chapter" data-level="1.6" data-path="textanalysis.html"><a href="textanalysis.html#tagging"><i class="fa fa-check"></i><b>1.6</b> Tagging</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="textprep.html"><a href="textprep.html"><i class="fa fa-check"></i><b>2</b> Preparing Textual Data</a><ul>
<li class="chapter" data-level="2.1" data-path="textprep.html"><a href="textprep.html#frequencies"><i class="fa fa-check"></i><b>2.1</b> Frequencies</a></li>
<li class="chapter" data-level="2.2" data-path="textprep.html"><a href="textprep.html#term-frequency"><i class="fa fa-check"></i><b>2.2</b> Term frequency</a></li>
<li class="chapter" data-level="2.3" data-path="textprep.html"><a href="textprep.html#tf-idf"><i class="fa fa-check"></i><b>2.3</b> Tf-idf</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="textprep" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Preparing Textual Data</h1>
<blockquote>
<p>Learning Objectives</p>
<ul>
<li>to come</li>
</ul>
</blockquote>
<hr />
<ul>
<li>DTM</li>
<li>n-grams</li>
<li>co-ocurrence</li>
</ul>
<p>First, we’ll load the libraries we need.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidytext)</code></pre>
<p>Let’s remind ourselves of what our data looks like.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words</code></pre>
<div id="frequencies" class="section level2">
<h2><span class="header-section-number">2.1</span> Frequencies</h2>
<p>Since our unit of analysis at this point is a word, let’s do some straightforward counting to figure out which words occur most frequently in the corpus as a whole.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</code></pre>
<p>We could start adding in a bit of visualization here. Let’s show the most frequent words that occur more than 2000 times.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">2000</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">reorder</span>(word, n)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>What if we’re interested in most used words per speech?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Count words by book</span>
doc_words &lt;-<span class="st"> </span>tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>)

<span class="co"># Calculate the total number of words by book and save them to a tibble</span>
total_words &lt;-<span class="st"> </span>doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">total =</span> <span class="kw">sum</span>(n))

<span class="co"># Join the total column with the rest of the data so we can calculate frequency</span>
doc_words &lt;-<span class="st"> </span><span class="kw">left_join</span>(doc_words, total_words)</code></pre>
<pre><code>#&gt; Joining, by = &quot;doc_id&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">doc_words </code></pre>
<p>Let’s graph the top words per book</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n, <span class="dt">fill =</span> doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>That’s cool looking, but let’s split it into facets so we can see by speech.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n, <span class="dt">fill =</span> doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>doc_id, <span class="dt">ncol =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We could keep cleaning this figure up by setting some minimum sizing, determining the spacing between y-axis labels better, and so forth, but now we’ll accept it as showing some sense of variation across speeches where certain words are used most.</p>
<p>What if we want to check the most highly common words per speech for a single president? We could filter this <code>doc_words</code> dataset based on the president’s name being in the doc_id, but I think it’s easier to filter from the initial tidy data and recount.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(president <span class="op">==</span><span class="st"> &quot;Barack Obama&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">20</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, n, <span class="dt">fill=</span>doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>doc_id, <span class="dt">ncol =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>))</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="term-frequency" class="section level2">
<h2><span class="header-section-number">2.2</span> Term frequency</h2>
<p>Sometimes, a raw count of a word is less important than understanding how often that word appears in respect to the total number of words in a text. This ratio would be the <strong>term frequency</strong>.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words &lt;-<span class="st"> </span>doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">term_freq =</span> n <span class="op">/</span><span class="st"> </span>total)

doc_words </code></pre>
<p>Let’s graph the term frequency for one of these speeches so we can understand the frequency distribution of words over a text.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(doc_id <span class="op">==</span><span class="st"> &quot;harry-s-truman-1946.txt&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(term_freq)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlim</span>(<span class="ot">NA</span>, <span class="fl">.012</span>)</code></pre>
<pre><code>#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<pre><code>#&gt; Warning: Removed 2 rows containing non-finite values (stat_bin).</code></pre>
<pre><code>#&gt; Warning: Removed 1 rows containing missing values (geom_bar).</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>This should make sense. Most words are used relatively rarely in a text. Only a few have a high term frequency.</p>
<p>We could keep filtering this data to see which terms have the high frequency, thus maybe increased significance, for different presidents and different particular speeches. We could also subset based on decade, and get a sense of what was important in each decade. We’re going to take a slightly different approach though. We’ve been looking at term frequency per document. What if we want to know about words that seem more important based on the contents of the entire corpus?</p>
</div>
<div id="tf-idf" class="section level2">
<h2><span class="header-section-number">2.3</span> Tf-idf</h2>
<p>For this, we can use term-frequency according to inverse document frequency (tf-idf). Tf-idf meansures how important a word is within a corpus by scaling term frequency per document according to the inverse of the term’s document frequency (how many documents within the corpus in which the term appears divided by the number of documents).</p>
<p>We could write our own function for tf-idf, but in this case we’ll take advantage of tidytext’s implementation.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words &lt;-<span class="st"> </span>doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, doc_id, n)

doc_words</code></pre>
<p>The tf-idf value will be:</p>
<ul>
<li>lower for words that appear in many documents in the corpus, and lowest when the word occurs in virtually all documents.</li>
<li>high for words that appear many times in few documents in the corpus, this lending high discrimiatory power to those doucments.</li>
</ul>
<p>Let’s look at some of the words in the corpus that have the highest tf-idf scores, which means words that are particularly distinctive for their documents.</p>
<pre class="sourceCode r"><code class="sourceCode r">doc_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>total) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))</code></pre>
<p>These results seem appropriate given our history. To understand the occurence of the years we might need to look more closely at the speeches themselves, and determine whether the years are significant or whether they need to be removed from the text. It might be that even if they don’t need to be removed from the text overall, they still need to be filtered out within the context of this analysis.</p>
<p>In the same way that we narrowed our analysis to Obama speeches earlier, we could subset the corpus before we calculate the tf-idf score to understand which words are most important for a single president within their sotu speeches. Let’s do that for Obama.</p>
<pre class="sourceCode r"><code class="sourceCode r">obama_tf_idf &lt;-<span class="st"> </span>tidy_sotu_words <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(president <span class="op">==</span><span class="st"> &quot;Barack Obama&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(doc_id, word, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_tf_idf</span>(word, doc_id, n) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(tf_idf))

obama_tf_idf</code></pre>
<p>Based on what you know of the Obama years and sotu speeches generally, how would you interpret these results?</p>
<p>Let’s try graphing these results, showing the top tf-idf terms per speech for Obama’s speeches.</p>
<pre class="sourceCode r"><code class="sourceCode r">obama_tf_idf <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word =</span> <span class="kw">factor</span>(word, <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">unique</span>(word)))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(doc_id) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">5</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(word, tf_idf, <span class="dt">fill =</span> doc_id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="st">&quot;tf-idf&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>doc_id, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.y =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">45</span>)) </code></pre>
<pre><code>#&gt; Warning in mutate_impl(.data, dots): Unequal factor levels: coercing to
#&gt; character</code></pre>
<pre><code>#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector

#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector

#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector

#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector

#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector

#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector

#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector

#&gt; Warning in mutate_impl(.data, dots): binding character and factor vector,
#&gt; coercing into character vector</code></pre>
<pre><code>#&gt; Selecting by tf_idf</code></pre>
<p><img src="R-text-analysis_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>TODO: document-term matrix<br />
TODO: length over time…other similar measures<br />
TODO: variation between the different presidents?<br />
TODO: say something about sentiment analysis and topic modeling</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="textanalysis.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cengel/R-text-analysis/edit/master/02-data-analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["R-text-analysis.pdf", "R-text-analysis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
