```{r, echo=FALSE, purl=FALSE, message = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,comment = "#>", purl = FALSE)
```

# Analyzing Texts {#textanalysis}


> Learning Objectives
>
> - perform different frequency counts and generate plots
> - use the `widyr` package to calculate co-ocurrance
> - use `igraph` and `ggraph` to plot a co-ocurrance graph
> - import and export a Document-Term Matrix into `tidytext`
> - use the `sentiments` dataset from `tidytext` to perform a sentiment analysis


------------


Now that we've read in our text and metadata, tokenized and cleaned it a little, let's move on to some analysis. 

First, we'll make sure we have loaded the libraries we'll need. 

```{r load-tidylibs, eval=FALSE}
library(tidyverse)
library(tidytext)
```

Let's remind ourselves of what our data looks like. 

```{r}
tidy_sotu_words
```

## Frequencies

Since our unit of analysis at this point is a word, let's do some straightforward counting to figure out which words occur most frequently in the corpus as a whole. 

```{r word-freq}
tidy_sotu_words %>%
  count(word, sort = TRUE)
```

We can pipe this into `ggplot` to make a graph of the words that occur more that 2000 times. We count the words and use `geom_col` to represent the n values.

```{r word-freq-plot}
tidy_sotu_words %>%
  count(word) %>% 
  filter(n > 2000) %>% 
  mutate(word = reorder(word, n)) %>%  # reorder values by frequency
  ggplot(aes(word, n)) +
     geom_col(fill = "gray") +
     coord_flip()  # flip x and y coordinates so we can read the words better
```


What if we want to check the most common words per speech for a single president and see which of the top words apppear in which speech?

```{r obama-topwords}
tidy_sotu_words %>%
  filter(president == "Barack Obama") %>%
  count(doc_id, word) %>%
  filter(n > 20) %>%
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill=doc_id)) +
    geom_col() +
    coord_flip()
```


>>> CHALLENGE: In any given year, how often is the word 'peace' used and how often is the word 'war' used? Make a bar chart that shows for each year the proportion of each of these two words out of the total of how often both words are used. Bonus: use decades instead of years.
 
```{r challenge-word-years, eval=FALSE, echo=FALSE}
# steps:
# Select only the words 'war' and 'peace'.
# count ocurrences of each per year
# plot n by year, and use position 'fill' to show the proportion

tidy_sotu_words %>%
  filter(word %in% c("war", "peace")) %>% 
  count(year, word) %>% 
  ggplot(aes(year, n, fill = word)) +
    geom_col(position = "fill")

# with decade instead of year
tidy_sotu_words %>%
  filter(word %in% c("war", "peace")) %>% 
  mutate(decade = floor(year/10) * 10) %>% 
  count(decade, word) %>% 
  ggplot(aes(decade, n, fill = word)) +
    geom_col(position = "fill")
```


As another example let us calculate the average number of words per speech for each president. How long was the average speech of each president and who are the top 'wordiest' presidents?  

First we summarize the words per president per speech

```{r word-president-count}
tidy_sotu_words %>%
  count(president, doc_id)
```

Then we use the output table and group it by president. That allows us to calculate the average number of words per speech.

```{r word-president-avg}
tidy_sotu_words %>%
  count(president, doc_id)  %>% 
  group_by(president) %>% 
  summarize(avg_words = mean(n)) %>% 
  arrange(desc(avg_words))
```


## Term frequency

Often a raw count of a word is less important than understanding how often that word appears relative to the total number of words in a text. This ratio would be the **term frequency**. We can use `dplyr` to calculate it like this:

```{r termfreq}
tidy_sotu_words %>%
  count(doc_id, word, sort = T)  %>%  # count n for each word
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot)
```

Let's graph the term frequency for one of these speeches so we can understand the frequency distribution of words over a text.

```{r termfreq-plot, message=FALSE}
tidy_sotu_words %>%
  filter(doc_id == "harry-s-truman-1946.txt") %>%
  count(doc_id, word)  %>%  # count n for each word
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot) %>% 
  ggplot(aes(term_freq)) +
    geom_histogram() 
```

This distribution makes sense. Most words are used relatively rarely in a text. Only a few have a high term frequency. 

We could keep filtering this data to see which terms have high frequency, thus maybe increased significance, for different presidents and different particular speeches. 

>>> CHALLENGE: Pick one president. For each of his speeches, which is the term with highest term frequency? Create a table as output. (Hint: `top_n`might be useful)

```{r challenge-termfreq, eval=FALSE, echo=FALSE}
tidy_sotu_words %>%
  filter(president == "Ronald Reagan") %>% 
  count(doc_id, word)  %>%  
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n), 
         term_freq = n/n_tot) %>% 
  top_n(1)
```


## Tf-idf 

So far we've been looking at term frequency per document. What if we want to know about words that seem more important based on the contents of the entire corpus?

For this, we can use **term-frequency according to inverse document frequency**, also callled **tf-idf**. Tf-idf measures how important a word is within a corpus by scaling term frequency per document according to the inverse of the term's document frequency (number of documents within the corpus in which the term appears divided by the number of documents). 

The tf-idf value will be:

- lower for words that appear in many documents in the corpus, and lowest when the word occurs in virtually all documents. 
- high for words that appear many times in few documents in the corpus, this lending high discriminatory power to those documents. 

The `tidytext` package includes a function `bind_tf_idf`. It takes a table that contains one-row-per-term-per-document, the name of the column that contains the words (terms), the name of the column which contains the doc-id, and the name of the column that contains the document-term counts.

So below we aggregate our tibble with the word tokens to create the one-row-per-term-per-document table and then pipe it into the `bind_tf_idf` function.

```{r tf-idf}
tidy_sotu_words %>%
  count(doc_id, word, sort = TRUE)  %>%  # aggregate to count n for each word
  bind_tf_idf(word, doc_id, n) 
```

We can see in the output that our function added three columns to our aggregated table: which contain term frequency, inverse document frequency and Tf-idf. 


Let's look at some of the words in the corpus that have the highest tf-idf scores, which means words that are particularly distinctive for their documents. 

```{r tf-idf-sort}
tidy_sotu_words %>%
  count(doc_id, word, sort = TRUE)  %>% 
  bind_tf_idf(word, doc_id, n) %>% 
  arrange(desc(tf_idf))
```

To understand the occurrence of the years as being particularly distinctive we might need to look more closely at the speeches themselves, and determine whether the years are significant or whether they need to be removed from the text either permanently in the clean up or temporarily using `filter`.


>>> Challenge: Pick the same president you chose above. For each of his speeches, which is the term with highest tf-idf? Create a table as output. What do you think about the differences? (Hint: Remember to group by doc_id before you use top_n)

```{r challenge-tfidf, eval=FALSE, echo=FALSE}
tidy_sotu_words %>%
  filter(president == "Ronald Reagan") %>%
  count(doc_id, word) %>%
  bind_tf_idf(word, doc_id, n) %>%
  group_by(doc_id) %>% 
  top_n(1)
```


## N-Grams

We mentioned n-grams in the intro, but let's revisit them here and take a look at the most common bigrams in the speeches. Remember this is what we get back:

```{r bigrams-unnest}
sotu_whole %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) # create bigram
```

Let's see the most common bigrams:

```{r bigrams-count}
sotu_whole %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  count(bigram, sort = TRUE) # count ocurrences and sord descending
```

Ok, so we again need to remove the stopwords. This time let's use dplyr's `filter` function for this. And before that we will `separate` the two words into two columns.

```{r bigrams-remove-stop}
sotu_bigrams <- sotu_whole %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separate into cols
  filter(!word1 %in% stop_words$word) %>% # remove stopwords
  filter(!word2 %in% stop_words$word)

sotu_bigrams %>% 
  count(word1, word2, sort = TRUE)
```

(Bonus question: What happened on that June 30th?)

A bigram can also be treated as a term in a document in the same way that we treated individual words. That means we can look at tf-idf values in the same way. 

First we will re-unite the two word columns again, and then generate the tf-idf count as above.

```{r bigram-tf-idf}
bigram_tf_idf <- sotu_bigrams %>%
  unite(bigram, word1, word2, sep = " ") %>% # combine columns
  count(president, bigram) %>%
  bind_tf_idf(bigram, president, n) %>%
  arrange(desc(tf_idf))
```


What makes the speeches of different presidents unique?

Let's pick a few presidents and plot their highest scoring tf-idf values here.

```{r bigram-tf-idf-plot}
potus <- c("John F. Kennedy", "Richard M. Nixon", "George Bush", "George W. Bush")

bigram_tf_idf %>%
  filter(president %in% potus) %>% 
  group_by(president) %>% 
  top_n(20) %>% 
  ggplot(aes(reorder(bigram, tf_idf), tf_idf, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~president, scales = "free", nrow = 2) +
  coord_flip()
```


## Co-occurrence

Co-occurrences give us a sense of words that appear in the same text, but not necessarily next to each other.

For this section we will make use of the `widyr` package. It allows us to turn our table into a wide matrix. In our case that matrix will be made up of the individual words and the cell values will be the counts of how many times they co-occur. Then we will turn the matrix back into a tidy form, where each row contains the word pairs and the count of their co-occurrence. This lets us count common pairs of words co-appearing within the same speech.

The function which helps us do this is the `pairwise_count()` function. 

Since processing the entire corpus would take too long here, we will only look at the last 20 words of each speech.


```{r pairwise-count}
library(widyr)

# extract last 100 words from text
sotu_whole$speech_end <- word(sotu_whole$text, -100, end = -1)

sotu_word_pairs <- sotu_whole %>% 
  unnest_tokens(word, speech_end) %>% 
  filter(!word %in% stop_words$word) %>% # remove stopwords
  pairwise_count(word, doc_id, sort = TRUE, upper = FALSE) # don't include upper triangle of matrix

sotu_word_pairs
```

To plot the co-occurrence network, we use the `igraph` library to convert our table into a network graph and `ggraph` which adds functionality to ggplot and makes it easier to create a network plot.

```{r plot-network} 
library(igraph)
library(ggraph)

sotu_word_pairs %>% 
  filter(n >= 10) %>%  # only word pairs that occur 10 or more times
  graph_from_data_frame() %>% #convert to graph
  ggraph(layout = "fr") + # place nodes according to the force-directed algorithm of Fruchterman and Reingold
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

There are alternative approaches for this as well. See for example the `findAssocs` function in the `tm` package.


## Document-Term Matrix

A [document-term matrix (DTM)](https://en.wikipedia.org/wiki/Document-term_matrix) is a format which is frequently used in text analysis. It is a matrix where we can see the counts of each term per document. In a DTM each row represents a document, each column represents a term, and the cell values are the counts of the occurrences of the term for the particular document.

`tidytext` provides functionality to convert to and from DTMs, if for example, your analyis requires specific functions that require you to use a different R package which only works with DTM objects.

The `cast_dtm` function can be used to create a DTM object from a tidy table.

Let's assume that for some reason we want to use the `findAssoc` function from the `tm` package.

First we use dplyr to create a table with the document name, the term, and the count.

```{r term-count}
# make a table with document, term, count
tidy_sotu_words %>% 
  count(doc_id, word) 
```

Now we cast it as a DTM.

```{r cast-dtm}
sotu_dtm <- tidy_sotu_words %>% 
  count(doc_id, word) %>% 
  cast_dtm(doc_id, word, n) 

class(sotu_dtm)
```

Finally, let's use it in the `tm` package.

```{r tm-dtm}
library(tm)

# look at the terms with tm function
Terms(sotu_dtm) %>% tail()

# most frequent terms
findFreqTerms(sotu_dtm, lowfreq = 5000)
              
# find terms associated with ...
findAssocs(sotu_dtm, "citizen", corlimit = 0.5)
```

Conversely, `tidytext` implements the `tidy` function (originally from the `broom` package) to import DocumentTermMatrix objects. Note that it only takes the cells from the DTM that are not 0, so there will be no rows with 0 counts.


## Sentiment analysis

`tidytext` comes with a dataset `sentiments` which contains several sentiment lexicons, where each word is attributed a certain sentiment, like this:

```{r sentiments}
sentiments
```

Here we will take a look at how the sentiment of the speeches change over time. We will use the lexicon from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), which assigns positive/negative labels for each word:

```{r bing}
bing_lex <- get_sentiments("bing")
bing_lex
```

Since this is a regular tibble, we can use these sentiments and join them to the words of our speeches. We will use `inner_join` from `dplyr`. Since our columns to join on have the same name (`word`) we don't need to explicitly name it.

```{r join-sentiments}
tidy_sotu_words %>% 
  inner_join(bing_lex) %>% # join
  count(year, sentiment) # group by year and sentiment
```

Finally we can visualize it like this:

```{r sentiment-plot, fig.width = 6, fig.height = 4}
tidy_sotu_words %>% 
  inner_join(bing_lex) %>% # join
  count(year, sentiment) %>% # group by year and sentiment
  ggplot(aes(year, n, color = sentiment)) +
    geom_line() +
    scale_x_continuous(breaks = seq(1790, 2016, by = 10)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

