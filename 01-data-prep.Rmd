```{r, echo=FALSE, purl=FALSE, message = FALSE}

library(sotu)
library(tidyverse)
library(tidytext)
library(readtext)

knitr::opts_chunk$set(warning=FALSE, message=FALSE,comment = "#>", purl = FALSE)
```

# Preparing Textual Data {#textprep}


> Learning Objectives
>
> - read textual data into R using `readtext`
> - use `stringr` package to manipulate strings
> - use `tidytext` functions to tokenize texts and remove stopwords
> - use `SnowballC` to stem words


------------

We'll use several R packages in this section: 

- `sotu` will provide the metadata and text of State of the Union speeches ranging from George Washington to Barack Obama. 
- `tidyverse` is a collection of R packages designed for data science, including `dplyr` with a set of verbs for common data manipulations and `ggplot2` for visualization. 
- `tidytext` provides specific functions for a "tidy" approach to working with textual data, where one row represents one "token" or meaningful unit of text, for example a word. 
- `readtext` provides a function well suited to reading textual data from a large number of formats into R, including metadata.  

```{r load-libs, eval=FALSE}
library(sotu)
library(tidyverse)
library(tidytext)
library(readtext)
```


## Reading text into R

First, let's look at the data in the `sotu` package. The metadata and texts come separately. Below is what the metadata look like. Can you tell how many speeches we have?

```{r sotu-meta}
# Let's take a quick look at the state of the union metadata
str(sotu_meta)
```

In order to work with the speech texts and to later practice reading text files from disk we're going to use a function `sotu_dir` to write the texts out. This function by default writes to a temporary directory with one speech in each file. It returns a character vector where each element is the name of the path to the individual speech file. We save this vector into the `file_paths` variable.

```{r file-paths}
# sotu_dir writes the text files to disk in a temporary dir, 
# but you could specific where you want them.
file_paths <- sotu_dir()
head(file_paths)
```

Now that we have the files on disk and a vector of filepaths, we can pass this vector directly into `readtext` to read the texts into a new variable.

```{r readtext}
# let's read in the files with readtext
sotu_texts <- readtext(file_paths)
head(sotu_texts)
```

To work with a single tabular dataset, we combine the text and metadata into a single tibble. You can see that our `sotu_texts` are organized by alphabetical order, so first we'll need to sort our metadata to match. 

```{r text-meta-combine}
sotu_whole <- 
  sotu_meta %>%  
  arrange(president) %>% # sort metadata
  bind_cols(sotu_texts) # combine with texts

glimpse(sotu_whole)
```

Now that we have our data, we need to think about cleaning it. Depending on the quality of your data, you might need to explicitly replace certain characters or words, remove urls or types of numbers, such as phone numbers, or otherwise clean up misspellings or errors. There are several ways to handle this sort of cleaning, we'll show a few examples for string manipulation and replacement. 


## String operations

R has many functions available to manipulate strings including functions like `grep` and `paste`, which come with the R base install. 

Here we will here take a look at the `stringr` package, which is part of the `tidyverse`. Under the hood it wraps a lot of the functions from the `stringi` package which is perhaps one of the most comprehensive string manipulation packages.

Below are examples for a few functions that might be useful.

`str_count` takes a characer vector as input and by default counts the number of pattern matches in a string. 

How man times does the word "citizen" appear in each of the speeches?

```{r str-count-citizen}
sotu_whole %>% 
  pull(text) %>% # extract texts vector
  str_count("citizen")
```

It is possible to use regular expressions, for example, this is how we would check how many times either "citizen" or "Citizen" appear in each of the speeches:

```{r str-count-citizen-regex}
sotu_whole %>% 
  pull(text) %>% # extract texts vector
  str_count("[C|c]itizen")
```

When used with the `boundary` argument `str_count` can count different entities like "character", "line_break", "sentence", or "word". Here we add a new column to the dataframe indicating how many words are there in each speech:

```{r str-word-count}
sotu_whole %>% 
  mutate(n_words = str_count(text, boundary("word"))) 
```

>>> CHALLENGE: Use the code above and add another column `n_sentences` where you calculate the number of sentences per speech. Then create a third column `avg_word_per_sentence`, where you calculate the number of words per sentence for each speech. Finally use `filter` to find which speech has shortest/longest average sentences length and what is the avderage length.

```{r avg-sentence-length, eval=FALSE, echo=FALSE}
sotu_whole %>% 
  mutate(n_words = str_count(text, boundary("word")),
         n_sentences = str_count(text, boundary("sentence")),
         avg_word_per_sentence = n_words/n_sentences) %>% 
  filter(avg_word_per_sentence %in% range(avg_word_per_sentence))
```


`str_detect` also looks for patterns, but instead of counts it returns a logical vector (TRUE/FALSE) indiciating if the pattern is or is not found. So we typically want to use it with the `filter` "verb" from `dplyr`.

What are the names of the documents where the words "citizen" and "Citizen" do **not** occur?

```{r str-detect}
sotu_whole %>% 
  filter(!str_detect(text, "[C|c]itizen")) %>% 
  select(doc_id) 
```

The `word` function extracts specific words from a character vector of words. By default it returns the first word. If for example we wanted to extract the first 5 words of each speech by Woodrow Wilson we provide the `end` argument like this:

```{r extract-words}
sotu_whole %>% 
  filter(president == "Woodrow Wilson") %>%  # sample a few speeches as demo
  pull(text) %>% # extract character vector
  word(end = 5) # end = 5 to extract words 1 - 5.
```

To clean this up a little we will first remove the newline characters (`\n`). We use  the `str_replace_all` function to replace all the ocurrences of the `\n` pattern with a white space `" "`. We need to add the escape character `\` in front of our pattern to be replaced so the backslash before the `n` is interpreted correctly.

```{r str-remove-newl}
sotu_whole %>% 
  filter(president == "Woodrow Wilson") %>%  
  pull(text) %>%
  str_replace_all("\\n", " ") %>% # replace newline
  word(end = 5) 
```

This looks better, but we still have a problem to extract exactly 5 words because of the whitespaces. So let's get rid of any whitespaces before and also of repeated whitespaces within the string with the convenient `str_squish` function.

```{r str-remove-spaces}
sotu_whole %>% 
  filter(president == "Woodrow Wilson") %>%  
  pull(text) %>%
  str_replace_all("\\n", " ") %>% 
  str_squish() %>%  # remove whitespaces
  word(end = 5) 
```


(For spell checks take a look at https://CRAN.R-project.org/package=spelling or https://CRAN.R-project.org/package=hunspell)


## Tokenize, lowercase

A very common part of data cleaning involves tokenization. While our data is already "tidy" insofar as each row is a single observation, a single text with metdata, the tidytext approach goes a step further to make each word it's own observation with metadata. We could write our own function to do this using a tokenizer, but `tidytext` provides a handy utility function just for this purpose.

```{r}
tidy_sotu <- sotu_whole %>%
  unnest_tokens(word, text)

tidy_sotu
```

Before we move on, we should note that the `unnest_tokens` function didn't just tokenize our texts at the word level. It also lowercased each word, and it could do quite a bit more. For instance, we could tokenize the text at the level of ngrams or sentences, if those are the best units of analysis for our work. We could also leave punctuation, which has been removed by default. Depending on what you need to do for analysis, you could do these operations during this step, or write custom functions and do it before you unnest tokens.

```{r}
# Word tokenization with punctuation
tidy_sotu_w_punct <- sotu_whole %>%
  unnest_tokens(word, text, strip_punct = FALSE)

tidy_sotu_w_punct

# Sentence tokenization
tidy_sotu_sentences <- sotu_whole %>%
  unnest_tokens(sentence, text, token = "sentences", to_lower = FALSE)

tidy_sotu_sentences

# N-gram tokenization
tidy_sotu_trigram <- sotu_whole %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

tidy_sotu_trigram
```


## Stopwords 

Another common type of cleaning in text analysis is to remove stopwords, or common words that theoretically provide less information about the content of a text. Depending on the type of analysis you're doing, you might leave these words in or use a highly curated list of stopwords. For now, as we move toward looking at words in documents based on frequency, we will remove some standard stopwords using a tidytext approach. 

First, let's look at the stopwords that tidytext gives us to get a sense of what they are.

```{r}
data(stop_words)
head(stop_words, n = 60)
```

You can see that we now have one word per row with associated metadata. We can now remove stopwords using an `anti-join`.

```{r, message=FALSE}
tidy_sotu_words <- tidy_sotu %>% 
  anti_join(stop_words)

tidy_sotu_words
```

We went from `r nrow(tidy_sotu)` to `r nrow(tidy_sotu_words)` rows, which means we had a lot of stopwords in our corpus. This is a huge removal, so for serious analysis, we might want to take a closer look at the stopwords and determine if we should use a different stopword list or otherwise create our own. 


## Word Stemming 

Another thing you may want to do is to stem your words, that is, to reduce them to their word stem or root form, like reducing *fishing*, *fished*, and *fisher* to the stem *fish*.
 
`tidytext` does not implement its own word stemmer. Instead it relies on separate packages like `hunspell` or `SnowballC`. 

We will give an example here for the `SnowballC` package. (`hunspell` appears to run much slower, and it also returns a list instead of a vector, so in this context `SnowballC` seems to be more convenient.)

```{r}
library(SnowballC)
tidy_sotu_words %>%
        mutate(word_stem = wordStem(word)) %>% head()
```

For lemmatization, you may want to take a look a the [`koRpus`](https://CRAN.R-project.org/package=koRpus) package, another [comprehensive R package for text analysis](https://cran.r-project.org/web/packages/koRpus/vignettes/koRpus_vignette.html). It allows to use [TreeTagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/), a widely used part-of-speech tagger. For full functionality of the R package a local installation of TreeTagger is recommended.

Now that we've read in our text and metadata, reshaped it a bit into the tidytext format, and cleaned it a bit while doing so, let's move on to some basic analysis. 

 